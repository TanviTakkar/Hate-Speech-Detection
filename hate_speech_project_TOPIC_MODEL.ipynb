{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\luvkumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luvkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luvkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luvkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from symspellpy import SymSpell\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from bertopic import BERTopic\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "\n",
    "# Download stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "# Download required resources for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "class HateSpeechAnalysis:\n",
    "    def __init__(self,data_path):\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.topic_model=BERTopic(language=\"English\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.sym_spell = self.initialize_symspell()\n",
    "        \n",
    "        \n",
    "    def preprocess_test_sentence(self,text, use_spacy=True, use_lemmatization=True, use_stemming=True):\n",
    "        # Convert to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        pattern = f\"[{re.escape(string.punctuation)}]\"\n",
    "        text=re.sub(pattern, '', text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if use_stemming:\n",
    "            # Apply stemming\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "        if use_lemmatization:\n",
    "            # Using NLTK for lemmatization\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Join tokens back to string\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "    def initialize_symspell(self):\n",
    "        sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "        dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "        term_index = 0  # column of the term names in the dictionary text file\n",
    "        count_index = 1   # column of the term frequencies in the dictionary text file\n",
    "        if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "            print(\"Dictionary file not found\")\n",
    "        return sym_spell\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data = self.data.sample(n=3000, random_state=42).reset_index(drop=True)\n",
    "        self.data['Content'] = self.data['Content'].apply(self.remove_html_tags)\n",
    "        self.data['Content'] = self.data['Content'].apply(self.remove_special_chars_and_digits)\n",
    "        self.data['Content'] = self.data['Content'].apply(self.correct_spellings)\n",
    "\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "\n",
    "    def remove_special_chars_and_digits(self, text):\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def correct_spellings(self, text):\n",
    "        suggestions = self.sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            return suggestions[0].term\n",
    "        else:\n",
    "            return text\n",
    "    \n",
    "    \n",
    "    def extract_topic_corrected(self):\n",
    "        sentences=self.data['Content'].apply(self.preprocess_test_sentence)\n",
    "        self.topic_model.fit(sentences)\n",
    "        self.topic_model.save(\"topic-model-for-hate-speech-correct\")\n",
    "        topic_model=BERTopic.load(\"topic-model-for-hate-speech-correct\")\n",
    "        topics, probs = self.topic_model.transform(sentences)\n",
    "        # Get the actual topic descriptions\n",
    "        topic_descriptions = topic_model.get_topic_info()\n",
    "        # Map topic IDs to their descriptions\n",
    "        return topics, topic_descriptions\n",
    "    \n",
    "        \n",
    "    def topic_append_with_content(self, topics, topic_desc):\n",
    "        topic_mapping = {row['Topic']: row['Name'] for _, row in topic_desc.iterrows()}\n",
    "            # Apply the mapping to get the topic text for each sentence\n",
    "        self.data['TopicText'] = [topic_mapping[topic] for topic in topics]\n",
    "\n",
    "        # Combine original text with topic text\n",
    "        self.data['CombinedText'] = self.data['Content'] + \" \" + self.data['TopicText']\n",
    "        pd.options.display.max_colwidth = 200\n",
    "        print(self.data['CombinedText'])\n",
    "\n",
    "    def extract_sentence_topic(self, sentence):\n",
    "        sentence=pd.DataFrame({'Content': [sentence]})\n",
    "        sentence=sentence['Content'].apply(self.preprocess_test_sentence)\n",
    "        topic_model=BERTopic.load('topic-model-for-hate-speech-correct')\n",
    "        topics, probs=topic_model.transform(sentence)\n",
    "        # Get the actual topic descriptions\n",
    "        topic_descriptions = topic_model.get_topic_info()\n",
    "\n",
    "        # Map topic IDs to their descriptions\n",
    "        topic_mapping = {row['Topic']: row['Name'] for _, row in topic_descriptions.iterrows()}\n",
    "\n",
    "        #print(topic_mapping)\n",
    "        #print(topic_mapping.get(topic[0],\"Unknown Topic\"))\n",
    "        topic=topic_mapping.get(topics[0],\"Unknow_Topic\")\n",
    "        \n",
    "        return topic\n",
    "\n",
    "    def preprocess_sentence(self,sentence):\n",
    "        sentence=pd.DataFrame({'Content':[sentence]})\n",
    "        sentence['Content'] = sentence['Content'].apply(self.remove_html_tags)\n",
    "        sentence['Content'] = sentence['Content'].apply(self.remove_special_chars_and_digits)\n",
    "        sentence['Content'] = sentence['Content'].apply(self.correct_spellings)\n",
    "        return sentence['Content'].tolist()[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analysis = HateSpeechAnalysis(data_path='HateSpeechDatasetBalanced.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing\n",
    "analysis.preprocess_data()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 13:22:31,255 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                    you should be deeply embarrassed by not fully recognizing how goddamn ignorant you sound and probably you are 0_articl_wikipedia_slut_like\n",
      "1                                                                                                                        do not make me make you fall in love with a bitch like me 0_articl_wikipedia_slut_like\n",
      "2                                                         trump america is anti immigrant sexual activity worker trans people insecure for tending hippo shift noah susa aids oral 0_articl_wikipedia_slut_like\n",
      "3                                                                                                        you guys are clearly a pole smoker please get run me over by a dump truck 0_articl_wikipedia_slut_like\n",
      "4       of come along the only reason people like strike complain is to get people like me banned so she and his barbara slime can run amok with their barbara the description slime aside from being ridicu...\n",
      "                                                                                                         ...                                                                                                   \n",
      "2995                                              a so what triggered it he a okay this snowflake you really are probably the only bitch here typical redneck male inbred rape sup 0_articl_wikipedia_slut_like\n",
      "2996                                                                                                                                        redirect talk carlos johnson musician 2_redirect_talk_archiv_asleep\n",
      "2997    wolf nix has given you a cookie cookies promote wiki love and hopefully this one has made your day better you can spread the by giving someone else a cookie whether it be someone you have had disa...\n",
      "2998                                                                                                                         i should endorse naming the attacking group of whites 0_articl_wikipedia_slut_like\n",
      "2999    replaceable fair use image venetian stiletto mpg thanks for uploading image venetian stiletto mpg i noticed the description page specifies that the media is being used under fair use but its use i...\n",
      "Name: CombinedText, Length: 3000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topics, topic_desc=analysis.extract_topic_corrected()\n",
    "analysis.topic_append_with_content(topics=topics, topic_desc=topic_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_articl_wikipedia_slut_like\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Please shut the door\"\n",
    "topic=analysis.extract_sentence_topic(sentence=sentence)\n",
    "print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
